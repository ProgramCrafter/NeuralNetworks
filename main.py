import traceback
import itertools
import random
import math

TRAIN_SPEED = 9e-5
TRAIN_LIMIT = 20
TRAIN_BLIMIT = 7e-3
COEF_LIMIT = 9999

from data_source import IconDataExtractor
from activators import TanhActivator
from utils import catch_nan

class InitialWeightsGenerator:
  INIT_WEIGHTS = [0.171, 0.204, 0.4, 0.468, 0.006, -0.911, -0.572, -0.471, 0.232, 0.623, -0.333, -0.613, 0.263, -0.155, 0.795, 0.644, 0.812, 0.688, 0.25, -0.464, -0.715, 0.148, 0.014, 0.375, -0.627, 0.434, 0.044, 0.803, -0.921, 0.111, 0.311, 0.128, -0.199, -0.977, -0.664, 0.195, -0.155, 0.546, 0.729, 0.596, 0.89, 0.669, 0.165, 0.96, -0.157, 0.666, -0.13, 0.59, 0.285, 0.284, -0.484, -0.145, 0.994, 0.769, -0.439, 0.219, 0.809, 0.8, 0.417, 0.607, 0.165, -0.811, -0.569, 0.272, 0.746, -0.222, -0.477, -0.762, -0.473, 0.376, 0.534, -0.645, 0.132, -0.831, -0.859, -0.039, -0.668, -0.944, -0.285, -0.581, 0.464, 0.121, 0.366, 0.661, -0.376, -0.453, 0.597, 0.07, -0.665, -0.769, -0.491, 0.87, -0.977, -0.097, -0.949, 0.405, 0.451, 0.791, 0.6, -0.997, -0.789, 0.443, -0.404, 0.378, -0.206, 0.893, 0.941, -0.03, 0.007, -0.546, 0.203, 0.265, -0.015, 0.835, 0.237, -0.703, -0.219, -0.675, -0.925, -0.874, -0.498, -0.834, -0.022, -0.411, 0.295, -0.428, -0.966, 0.227, 0.01, 0.106, 0.695, -0.523, 0.125, -0.19, 0.836, 0.584, -0.756, -0.091, -0.018, -0.482, -0.965, -0.225, -0.339, 0.343, -0.991, -0.25, 0.4, 0.31, -0.491, 0.423, -0.03, -0.621, -0.134, 0.06, -0.665, -0.089, -0.452, -0.441, -0.193, 0.411, -0.619, -0.515, 0.999, -0.337, 0.909, 0.963, 0.856, -0.847, 0.7, -0.909, -0.562, -0.81, -0.496, -0.5, -0.959, 0.811, 0.424, -0.207, 0.121, -0.885, 0.306, -0.815, 0.084, -0.125, 0.135, 0.336, 0.234, -0.329, 0.543, -0.053, -0.398, -0.147, -0.431, 0.693, 0.863, -0.233, -0.333, 0.977, -0.568, 0.697, -0.787, 0.223, 0.438, 0.598, 0.319, 0.276, -0.183, 0.255, -0.822, 0.892, 0.663, -0.746, -0.692, 0.898, 0.325, 0.732, -0.092, 0.373, -0.774, -0.812, 0.677, 0.961, -0.667, -0.01, 0.421, -0.947, 0.601, 0.55, -0.715, -0.333, 0.883, 0.218, 0.813, -0.636, -0.125, -0.925, -0.023, -0.822, -0.432, 0.811, 0.128, 0.085, -0.11, 0.053, -0.119, 0.599, -0.694, 0.857, -0.075, -0.364, -0.774, 0.837, 0.691, 0.264, -0.207, 0.657, -0.614, 0.051, 0.958, 0.26, -0.782, 0.747, -0.947, 0.825, -0.154, 0.633, -0.139, -0.494, -0.661, 0.62, 0.042, 0.913, 0.963, 0.28, -0.02, -0.541, 0.285, -0.094, 0.413, 0.491, -0.879, 0.993, 0.256, -0.945, 0.76, 0.905, 0.241, -0.897, -0.156, -0.471, 0.246, 0.162, -0.682, -0.6, 0.722, 0.483, -0.993, -0.495, -0.858, -0.89, -0.141, -0.962, 0.14, 0.426, 0.256, 0.155, 0.13, 0.318, 0.196, 0.021, -0.254, 0.637, 0.187, -0.955, 0.28, -0.013, 0.682, -0.344, -0.653, -0.178, -0.528, 0.562, 0.78, -0.058, -0.615, -0.033, -0.686, -0.21, 0.437, -0.654, 0.427, 0.364, 0.82, 0.979, 0.82, 0.354, -0.006, 0.079, -0.643, -0.035, -0.58, -0.926, 0.951, -0.247, 0.039, -0.245, -0.145, 0.392, 0.853, 0.335, -0.851, 0.512, 0.184, -0.625, -0.214, -0.077, -0.865, -0.333, 0.841, 0.875, 0.519, 0.244, 0.501, 0.691, 0.438, 0.488, 0.996, 0.254, 0.016, -0.259, -0.726, 0.56, 0.886, 0.231, -0.451, 0.24, 0.096, -0.614, -0.02, 0.688, -0.27, -0.056, -0.525, -0.941, -0.078, -0.413, -0.879, -0.817, -0.682, 0.889, -0.388, 0.005, 0.764, -0.87, 0.944, 0.773, -0.682, -0.463, -0.147, -0.175, 0.517, 0.24, 0.072, 0.783, -0.848, -0.733, -0.26, 0.626, 0.745, 0.32, 0.42, -0.548, 0.157, -0.573, -0.327, -0.303, -0.357, 0.217, 0.674, -0.828, 0.562, -0.812, 0.791, 0.303, 0.728, 0.741, -0.11, 0.09, 0.894, -0.375, -0.232, -0.53, -0.35, 0.935, 0.289, -0.81, -0.163, -0.366, -0.718, -0.326, -0.505, 0.435, 0.035, 0.411, 0.253, 0.277, -0.079, 0.541, 0.499, -0.528, -0.708, 0.92, -0.625, -0.519, -0.141, 0.023, -0.534, 0.68, -0.402, 0.917, -0.386, -0.343, 0.784, 0.604, -0.388, -0.285, -0.659, -0.17, -0.286, -0.675, 0.616, -0.005, -0.808, 0.802, 0.88, 0.096, -0.635, -0.121, 0.53, -0.31, 0.143, -0.054, -0.749, -0.755, -0.327, 0.327, -0.603, 0.393, 0.871, 0.275, 0.934, -0.064, 0.695, 0.679, 0.0, -0.231, 0.5, -0.922, 0.343, -0.513, -0.44, -0.433, -0.621, -0.747, -0.606, 0.221, -0.549, 0.747, 0.921, 0.87, -0.553, -0.83, 0.94, 0.897, 0.081, -0.669, 0.528, 0.812, 0.625, -0.441, -0.987, -0.587, -0.288, 0.603, -0.481, -0.903, 0.599, 0.445, -0.194, -0.557, -0.604, 0.389, 0.246, -0.926, -0.49, 0.42, 0.091, 0.343, -0.757, 0.371, -0.591, 0.315, -0.357, 0.51, 0.509, 0.448, -0.762, 0.014, 0.512, 0.522, 0.261, -0.149, 0.516, -0.295, 0.626, 0.016, 0.891, -0.622, -0.536, 0.677, -0.013, 0.581, -0.601, 0.329, -0.295, -0.062, -0.288, -0.226, -0.462, 0.577, -0.746, 0.829, -0.765, 0.773, -0.987, -0.791, -0.033, 0.995, 0.745, -0.788, -0.182, -0.512, 0.128, 0.05, 0.887, 0.915, -0.533, -0.237, 0.766, -0.772, -0.727, -0.385, -0.648, -0.246, -0.364, -0.912, 0.288, 0.033, 0.38, 0.304, 0.598, -0.141, -0.149, -0.702, 0.367, 0.451, 0.958, -0.116, 0.064, 0.132, 0.104, 0.669, 0.864, 0.506, 0.225, -0.639, -0.334, -0.45, 0.943, -0.17, 0.339, 0.278, 0.28, 0.335, 0.863, 0.809, 0.036, 0.261, 0.395, 0.093, -0.771, 0.232, 0.513, -0.809, 0.786, 0.204, -0.253, 0.072, 0.818, -0.588, -0.595, -0.114, -0.025, 0.184, 0.075, 0.774, -0.475, 0.641, -0.815, 0.286, -0.139, -0.415, 0.125, 0.218, -0.978, -0.814, -0.781, 0.054, 0.164, 0.933, 0.557, 0.864, 0.49, 0.688, -0.219, -0.804, 0.212, -0.834, -0.676, 0.127, -0.1, -0.411, 0.232, -0.701, 0.862, -0.577, -0.422, 0.231, -0.866, 0.527, -0.168, -0.055, -0.802, -0.338, -0.952, -0.558, -0.093, 0.35, -0.95, 0.337, -0.546, -0.199, 0.073, -0.018, -0.909, 0.655, 0.112, 0.578, 0.317, -0.124, 0.433, -0.324, -0.207, -0.652, 0.999, 0.572, -0.41, 0.934, -0.985, 0.498, -0.531, 0.069, 0.641, -0.483, 0.132, 0.506, 0.524, 0.858, -0.085, -0.48, -0.766, 0.092, -0.733, 0.436, -0.985, 0.571, 0.455, -0.739, 0.632, 0.704, -0.104, 0.766, -0.672, -0.577, -0.078, -0.496, -0.071, 0.133, 0.662, -0.306, 0.775, 0.98, 0.089, -0.344, -0.242, 0.388, 0.261, 0.585, 0.974, 0.132, 0.263, 0.944, 0.251, -0.636, 0.653, 0.208, 0.547, 0.67, 0.339, 0.7, -0.951, -0.085, 0.14, 0.477, 0.71, -0.513, -0.474, 0.848, 0.281, 0.703, -0.406, -0.107, 0.624, -0.305, -0.129, 0.529, 0.273, 0.284, 0.229, 0.956, 0.345, 0.918, 0.362, 0.22, 0.389, 0.85, 0.677, -0.062, 0.398, 0.767, -0.768, -0.305, 0.483, 0.348, -0.921, 0.091, 0.035, 0.086, 0.224, 0.541, -0.601, -0.298, -0.623, 0.072, 0.588, -0.083, 0.685, -0.427, -0.604, -0.663, 0.261, 0.162, -0.818, -0.691, -0.502, -0.747, -0.056, -0.998, 0.006, 0.832, 0.055, 0.859, 0.966, -0.102, -0.783, -0.829, 0.083, 0.492, -0.038, -0.306, -0.5, -0.355, -0.3, -0.445, 0.391, -0.619, -0.272, -0.998, 0.086, 0.002, -0.586, 0.807, -0.894, -0.019, -0.51, -0.453, -0.516, 0.482, -0.474, 0.379, 0.683, 0.321, 0.603, 0.64, 0.193, -0.84, -0.678, -0.364, -0.594, 0.855, -0.923, -0.054, -0.525, -0.937, -0.282, -0.237, -0.949, 0.496, 0.019, 0.247, -0.937, 0.502, -0.488, -0.023, 0.221, 0.084, -0.06, -0.023, -0.317, 0.758, -0.631, 0.864, 0.04, 0.842, -0.291, -0.607, -0.103, 0.158, 0.485, -0.127, -0.95, -0.835, 0.175, 0.056, -0.933, -0.901, 0.453, 0.204, 0.003, 0.742, -0.455, -0.783, -0.738, 0.341, -0.153, 0.009, 0.068, -0.78, -0.91, 0.663, -0.516, -0.623, 0.198, -0.429, 0.473, 0.501, -0.126, -0.448, 0.011, 0.588, 0.072, -0.566, 0.467, 0.905, 0.699, 0.458, -0.729, 0.723, -0.419, -0.258, 0.313, 0.815, -0.715, 0.384, 0.063, -0.288, -0.772, -0.585, -0.451, 0.771, 0.698, -0.935, 0.285, 0.82, 0.169, 0.544, -0.126, -0.255, 0.138, -0.69, -0.76, -0.825, -0.633, -0.836, -0.684, 0.074, 0.771, 0.862, -0.756, -0.1, 0.221, -0.041, -0.597, 0.271, -0.263, -0.86, 0.081, 0.163, -0.379, 0.832, 0.809, -0.208, -0.766, -0.805, -0.965, 0.792, -0.578, 0.839, 0.471, 0.79, 0.303, -0.914, 0.498, 0.982, -0.679, 0.489, -0.131, -0.721, -0.581, 0.936, -0.138, 0.32, 0.502, 0.85, -0.334, 0.524, 0.861, 0.282, -0.495, -0.117, -0.903, 0.353, -0.465, 0.524, -0.826, 0.655, 0.781, 0.349, 0.858, -0.977, -0.86, -0.353, -0.204, 0.235, 0.154, -0.51, -0.408, -0.26, -0.381, 0.909, -0.03, 0.257, -0.169, -0.244, 0.452, 0.081, 0.817, 0.725, -0.344, -0.195, -0.579, 0.267, -0.975, 0.41, -0.908, 0.517, 0.76, -0.705, -0.266, 0.784, -0.952, -0.052, 0.957, -0.273, -0.641, -0.639, -0.881, -0.316, 0.158, -0.758, 0.838, -0.66, -0.469, 0.699, 0.613, -0.528, 0.439, 0.635, -0.108, -0.368, 0.613, -0.028, -0.125, 0.087, 0.0, -0.785, -0.814, 0.059, -0.121, -0.371, -0.76, 0.524, -0.442, -0.542, -0.706, -0.998, 0.613, 0.432, 0.888, -0.011, -0.4, -0.472, -0.003, -0.527, 0.929, 0.596, -0.593, 0.269, -0.167, 0.277, 0.893, 0.62, -0.861, 0.429, -0.223, 0.529, -0.015, 0.953, 0.328, 0.805, 0.59, 0.092, -0.549, -0.505, 0.071, 0.221, 0.526, 0.355, -0.665, -0.127, -0.375, 0.634, -0.903, -0.411, 0.884, 0.01, -0.671, -0.839, -0.248, -0.971, -0.875, -0.523, 0.345, 0.096, 0.581, 0.51, -0.667, 0.516, -0.391, -0.531, -0.618, 0.182, 0.582, 0.037, 0.078, -0.687, -0.769, 0.749, 0.07, 0.061, 0.229, -0.061, -0.205, -0.963, 0.605, -0.646, -0.028, 0.019, 0.947, -0.175, 0.017, -0.996, 0.876, 0.063, 0.808, -0.327, 0.102, 0.054, -0.153, -0.375, 0.106, -0.019, 0.806, -0.635, 0.285, 0.46, 0.547, 0.983, -0.144, 0.924, -0.536, -0.354, -0.515, -0.715, 0.231, -0.046, -0.718, 0.171, -0.022, 0.103, -0.587, -0.367, -0.055, 0.054, 0.324, 0.225, 0.269, 0.805, -0.962, 0.563, 0.646, -0.73, -0.192, 0.923, -0.409, 0.922, -0.449, 0.524, -0.95, 0.197, -0.168, -0.677, 0.017, 0.557, 0.024, -0.968, 0.096, 0.994, 0.008, 0.236, -0.664, 0.331, 0.155, 0.97, 0.706, -0.014, -0.259, -0.034, 0.58, 0.046, -0.269, 0.045, 0.355, 0.668, 0.745, 0.594, -0.994, 0.914, 0.283, -0.34, 0.069, -0.983, -0.03, -0.241, 0.869, 0.895, 0.58, -0.171, -0.794, -0.06, -0.695, 0.287, -0.154, 0.423, -0.032, -0.455, -0.68, -0.262, -0.464, -0.593, -0.193, -0.982, -0.692, 0.096, 0.769, -0.394, 0.871, 0.849, 0.786, -0.804, 0.511, -0.252, -0.993, 0.765, 0.731, 0.631, -0.644, -0.029, 0.257, 0.799, -0.031, -0.261, 0.377, 0.79, -0.288, -0.723, 0.73, -0.161, -0.115, -0.961, 0.451, -0.001, -0.524, 0.479, -0.442, 0.422, -0.98, -0.466, 0.599, 0.876, 0.506, 0.405, -0.017, -0.011, 0.815, -0.262, 0.445, 0.844, 0.562, -0.547, 0.661, -0.331, -0.345, -0.351, -0.781, 0.401, -0.324, -0.192, 0.026, 0.542, -0.904, 0.484, -0.364, 0.77, -0.994, -0.656, -0.136, 0.329, 0.384, -0.353, 0.571, 0.607, 0.118, 0.302, -0.8, -0.437, -0.104, -0.493, 0.993, 0.78, 0.278, -0.838, -0.104, -0.464, 0.061, -0.241, -0.15, 0.405, 0.73, 0.078, 0.9, 0.591, -0.19, 0.594, 0.015, -0.009, 0.714, -0.728, -0.197, 0.123, -0.809, -0.37, -0.164, -0.807, 0.976, 0.899, 0.261, 0.927, 0.356, -0.75, -0.724, 0.48, -0.779, -0.612, -0.679, -0.563, 0.922, 0.354, 0.585, 0.807, 0.026, 0.696, -0.783, 0.531, 0.01, 0.027, 0.476, 0.15, -0.494, -0.477, 0.803, -0.423, 0.004, 0.372, -0.015, -0.413, -0.064, 0.416, 0.799, 0.927, -0.253, -0.922, -0.847, -0.716, -0.75, 0.756, 0.605, 0.81, -0.338, 0.513, 0.297, -0.317, 0.599, 0.387, -0.244, -0.729, 0.575, 0.209, 0.071, 0.835, -0.739, 0.823, -0.013, -0.504, -0.543, 0.453, 0.789, 0.619, -0.14, -0.155, 0.452, 0.323, 0.394, 0.797, 0.056, 0.816, -0.313, 0.696, -0.809, -0.06, 0.99, 0.864, -0.822, 0.921, 0.523, -0.712, 0.699, 0.49, 0.072, 0.959, 0.895, -0.052, -0.804, 0.228, 0.143, 0.433, 0.45, 0.597, 0.431, -0.689, 0.609, -0.617, 0.264, -0.568, -0.634, -0.839, -0.571, 0.846, -0.429, -0.924, 0.72, -0.332, 0.353, -0.773, -0.298, 0.348, 0.626, 0.699, -0.664, -0.895, 0.377, 0.391, 0.137, -0.05, -0.674, -0.536, -0.314, -0.424, -0.61, -0.812, 0.273, 0.355, -0.16, -0.946, -0.692, -0.317, 0.399, -0.382, -0.074, -0.424, 0.093, 0.847, -0.115, 0.312, 0.528, -0.248, -0.198, 0.274, 0.962, 0.74, -0.775, 0.477, -0.925, 0.675, 0.527, 0.804, -0.645, 0.123, -0.131, 0.926, 0.706, 0.052, 0.373, 0.956, -0.311, -0.225, 0.194, 0.446, -0.486, 0.618, -0.529, 0.869, -0.872, 0.377, 0.708, 0.331, 0.46, -0.037, -0.032, 0.264, 0.078, -0.62, 0.707, -0.131, 0.681, -0.04, 0.693, -0.338, 0.763, -0.744, -0.694, -0.152, -0.949, 0.522, -0.033, 0.633, 0.905, -0.991, -0.008, -0.05, 0.64, -0.702, -0.527, 0.926, 0.324, 0.92, 0.92, -0.736, 0.011, -0.165, -0.039, 0.847, -0.668, 0.143, 0.292, -0.413, -0.584, 0.635, -0.358, 0.56, 0.378, -0.812, -0.729, 0.827, -0.393, 0.568, 0.096, -0.706, -0.943, 0.213, 0.152, -0.65, 0.525, -0.561, 0.77, -0.688, 0.962, -0.965, 0.633, -0.334, 0.542, -0.206, 0.542, -0.649, -0.87, 0.23, -0.847, 0.612, 0.018, 0.519, -0.967, 0.043, -0.486, 0.502, -0.723, 0.309, -0.866, -0.046, -0.584, -0.74, 0.603, 0.787, -0.435, -0.088, -0.593, 0.151, -0.969, -0.714, 0.629, 0.521, -0.917, -0.221, 0.348, 0.847, 0.619, 0.173, 0.698, -0.122, -0.181, 0.085, -0.395, 0.181, -0.732, -0.952, 0.767, 0.657, -0.24, 0.607, 0.79, -0.868, -0.817, -0.957, 0.087, -0.588, 0.726, -0.545, -0.457, 0.487, 0.947, -0.979, -0.133, -0.095, -0.33, -0.567, 0.241, -0.601, -0.239, -0.45, -0.22, 0.176, -0.29, 0.309, 0.972, -0.796, -0.41, 0.445, 0.132, 0.943, 0.024, 0.809, 0.453, -0.113, 0.864, 0.709, -0.702, 0.221, -0.193, 0.792, 0.437, -0.878, 0.746, -0.79, 0.863, -0.824, -0.622, 0.427, -0.704, 0.826, 0.305, 0.17, 0.642, -0.508, -0.178, 0.712, -0.845, -0.846, -0.254, -0.596, 0.887, 0.658, 0.738, -0.516, -0.311, -0.049, -0.442, 0.651, -0.375, -0.86, -0.357, 0.78, -0.582, 0.393, 0.116, 0.963, -0.678, -0.559, 0.053, 0.378, -0.276, 0.674, 0.465, 0.494, -0.566, -0.929, -0.993, -0.12, -0.284, -0.244, 0.982, 0.209, 0.465, 0.753, 0.055, 0.988, -0.533, 0.088, -0.494, 0.723, -0.093, 0.537, 0.199, 0.305, -0.448, -0.506, 0.482, 0.893, 0.604, 0.225, 0.261, -0.183, 0.701, -0.128, -0.891, -0.934, 0.559, 0.048, -0.203, 0.452, 0.616, 0.045, -0.674, 0.754, 0.017, -0.965, -0.462, 0.278, -0.375, 0.995, -0.1, 0.703, 0.623, 0.468, -0.152, -0.547, -0.166, -0.453, 0.227, 0.408, 0.631, -0.52, 0.47, -0.174, -0.223, -0.657, 0.538, -0.89, -0.332, 0.78, 0.089, 0.91, -0.978, -0.356, 0.292, -0.609, -0.711, 0.354, -0.563, -0.553, 0.4, -0.514, -0.276, -0.377, -0.906, 0.456, -0.299, 0.594, -0.854, -0.056, 0.651, 0.195, 0.402, 0.169, -0.689, -0.603, 0.42, 0.885, -0.551, 0.563, -0.138, -0.206, -0.488, 0.508, 0.018, 0.627, -0.197, 0.615, -0.518, 0.51, -0.826, -0.014, 0.612, 0.205, 0.125, -0.959, 0.518, -0.81, 0.646, -0.646, 0.024, -0.004, 0.974, 0.62, 0.429, -0.55, 0.218, -0.271, -0.32, -0.791, 0.822, 0.563, -0.31, -0.66, -0.131, -0.812, 0.249, -0.556, 0.018, -0.703, -0.739, 0.208, -0.203, -0.511, -0.022, -0.63, 0.109, 0.059, -0.718, -0.485, 0.009, -0.166, 0.18, -0.589, 0.949, -0.297, 0.101, -0.627].__iter__()
  
  def generate(self, iterable):
    if not self.INIT_WEIGHTS:
      return [random.random() * 2 - 1 for it in iterable]
    else:
      return [random.random() * 0 + self.INIT_WEIGHTS.__next__() for it in iterable]

class INeuron:
  def __init__(self, activator, initializer, previous_layer): pass
  def calculate(self):       pass
  def sprintf_weights(self): pass

class InputValue(INeuron):
  def __init__(self, activator, initializer, previous_layer):
    self.activator = activator
    self.coef = initializer.generate(' ')[0]
    self.value = 0
    self.cache = None
    self.coefs = [self.coef]
    self.next_layer = []
  
  @catch_nan
  def calculate(self):
    if not self.cache:
      self.cache = self.activator.result(self.value)
    return self.cache
  
  @catch_nan
  def delta_as_last(self, error):
    s = self.coef * self.value
    d = self.activator.derivative(s)
    
    return d * error
  
  @catch_nan
  def delta_as_not_last(self, next_deltas):
    s = self.coef * self.value
    d = self.activator.derivative(s)
    
    mult = 0
    for i, neuron in enumerate(self.next_layer):
      mult += neuron[0].coefs[neuron[1]] * next_deltas[i]
    
    return d * mult
  
  @catch_nan
  def delta(self, next_deltas):
    if self.next_layer:
      a = self.delta_as_not_last(next_deltas)
    else:
      a = self.delta_as_last(next_deltas)
    
    train_value = TRAIN_SPEED * a * self.value
    
    if train_value < -TRAIN_LIMIT: train_value = -TRAIN_LIMIT
    elif train_value > TRAIN_LIMIT: train_value = TRAIN_LIMIT
    
    k = self.coef - train_value
    if k < -COEF_LIMIT: k = -COEF_LIMIT
    elif k > COEF_LIMIT: k = COEF_LIMIT
    self.coef = k
    self.coefs = [k]
    self.cache = None
    
    return a

class Neuron(INeuron):
  def __init__(self, activator, initializer, previous_layer):
    self.activator = activator
    self.previous_layer = previous_layer
    self.next_layer = []
    self.coefs = initializer.generate(previous_layer)
    self.cache = None
    
    for i,p in enumerate(self.previous_layer):
      p.next_layer.append((self, i))
  
  @catch_nan
  def calculate(self):
    if not self.cache:
      s = sum(self.coefs[i] * v.calculate() for i, v in enumerate(self.previous_layer))
      self.cache = self.activator.result(s)
    
    return self.cache
  
  @catch_nan
  def delta_as_last(self, error):
    s = sum(self.coefs[i] * v.calculate() for i, v in enumerate(self.previous_layer))
    
    d = self.activator.derivative(s)
    
    return d * error
  
  @catch_nan
  def delta_as_not_last(self, next_deltas):
    s = sum(self.coefs[i] * v.calculate() for i, v in enumerate(self.previous_layer))
    
    d = self.activator.derivative(s)
    
    mult = 0
    for i, neuron in enumerate(self.next_layer):
      mult += neuron[0].coefs[neuron[1]] * next_deltas[i]
    
    return d * mult
  
  @catch_nan
  def delta(self, next_deltas):
    if self.next_layer:
      a = self.delta_as_not_last(next_deltas)
    else:
      a = self.delta_as_last(next_deltas)
    
    for i, prev_neuron in enumerate(self.previous_layer):
      train_value = TRAIN_SPEED * a * prev_neuron.calculate()
      
      if train_value < -TRAIN_LIMIT: train_value = -TRAIN_LIMIT
      elif train_value > TRAIN_LIMIT: train_value = TRAIN_LIMIT
      
      k = self.coefs[i] - train_value
      if k < -COEF_LIMIT: k = -COEF_LIMIT
      elif k > COEF_LIMIT: k = COEF_LIMIT
      self.coefs[i] = k
    
    self.cache = None
    
    return a

class SparelinkNeuralNetwork:
  def __init__(self, activator, initializer, levels):
    self.activator_prefix = str(activator)
    
    self.layers = [[InputValue(activator, initializer, None) for i in range(4 ** levels * 3)]]
    
    # first layer (4 ** levels == 256) - neurons with 3 inputs (R, G, B)
    self.layers.append(
      [Neuron(activator, initializer, self.layers[0][i*3:i*3+3]) for i in range(4 ** levels)]
    )
    
    # second layer (4 ** 3 == 64)
    # third layer  (4 ** 2 == 16)
    # fourth layer (4 ** 1 == 4)
    # final layer  (4 ** 0 == 1)
    for i in range(levels)[::-1]:
      last_layer = self.layers[-1]
      size = 4 ** i
      
      self.layers.append([Neuron(activator, initializer, [
        last_layer[k], last_layer[k + size], last_layer[k + 2 * size], last_layer[k + 3 * size]
      ]) for k in range(size)])
    
  def set_inputs(self, input_values):
    for i, neuron in enumerate(self.layers[0]):
      neuron.value = input_values[i]
    
    for layer in self.layers:
      for neuron in layer:
        neuron.cache = None
  
  def calculate(self):
    for neuron in self.layers[-1]:
      yield neuron.calculate()
  
  def train(self, wanted):
    # back errors propagation
    output = list(self.calculate())
    
    # output layer
    deltas = [
      neuron.delta(output[i] - wanted[i])
          for i, neuron in enumerate(self.layers[-1])
    ]
    
    for layer in self.layers[:-1][::-1]:
      deltas = [neuron.delta(deltas) for neuron in layer]
  
  def enum_weights(self):
    for layer in self.layers:
      for neuron in layer:
        yield from neuron.coefs

def epoch(net, data):
  sum_sq = 0
  
  cases = list(range(data.cases()))
  random.shuffle(cases)
  
  trains = data.cases() // 2
  
  for case in cases:
    net.set_inputs(data.extract_data(case))
    
    net_result = net.calculate()
    wanted_result = data.wanted(case)
    
    for i, nr in enumerate(net_result):
      sum_sq += (nr - wanted_result[i]) * (nr - wanted_result[i])
    
    if trains > 0:
      net.train(wanted_result)
      trains -= 1
    
  return sum_sq / data.cases()

def main():
  try:
    random.seed(0x14609A25)
    
    net = SparelinkNeuralNetwork(TanhActivator(), InitialWeightsGenerator(), 4)
    data = IconDataExtractor(__file__ + '/../icons/r-',
      ['oo-0-0.png',   'tg-0-0.png',   'ya-0-0.png',
       'oo-90-0.png',  'tg-90-0.png',  'ya-90-0.png',
       'oo-180-0.png', 'tg-180-0.png', 'ya-180-0.png',
       'oo-270-0.png', 'tg-270-0.png', 'ya-270-0.png',
      ])
    
    print(net, data)
    last_distance = epoch(net, data)
    
    try:
      for i in range(100001):
        cur_distance = epoch(net, data)
        
        if i % 200 == 0:
          print('Epoch %6d - square distance = %.4f (delta = %.4f)' % (i, cur_distance, cur_distance - last_distance))
          last_distance = min(last_distance, cur_distance)
        
        if cur_distance < 0.005:
          break
    except KeyboardInterrupt:
      pass
    except Exception:
      raise
    
    print('\nResults:')
    
    sum_distance = 0
    sum_abs = 0
    
    for case in range(data.cases()):
      net.set_inputs(data.extract_data(case))
      
      net_result = list(net.calculate())
      wanted_result = data.wanted(case)
      
      print('Net output = %.4f; wanted = %d' % (net_result[0], wanted_result[0]))
      
      for i, nr in enumerate(net_result):
        sum_distance += (nr - wanted_result[i]) * (nr - wanted_result[i])
        sum_abs += abs(nr - wanted_result[i])
    
    print('\nMedian square error: %.4f' % (sum_distance / data.cases()))
    print('Median absolute error: %.4f' % (sum_abs / data.cases()))
    
    print('\nWeights:')
    print([int(w * 1000) / 1000 for w in net.enum_weights()])
  except:
    traceback.print_exc()

main()
input()
