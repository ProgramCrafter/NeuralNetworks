import traceback
import itertools
import random
import math

TRAIN_SPEED = 3e-4
TRAIN_LIMIT = 20
TRAIN_BLIMIT = 7e-3
COEF_LIMIT = 9999

from data_source import IconDataExtractor
from activators import TanhActivator
from utils import catch_nan

class InitialWeightsGenerator:
  INIT_WEIGHTS = [0.171, 0.204, 0.399, 0.467, 0.006, -0.911, -0.572, -0.47, 0.232, 0.623, -0.332, -0.612, 0.263, -0.154, 0.795, 0.644, 0.812, 0.688, 0.25, -0.464, -0.714, 0.148, 0.013, 0.374, -0.626, 0.434, 0.043, 0.803, -0.921, 0.111, 0.311, 0.128, -0.199, -0.977, -0.664, 0.195, -0.154, 0.545, 0.729, 0.595, 0.889, 0.668, 0.165, 0.96, -0.157, 0.665, -0.13, 0.589, 0.284, 0.283, -0.483, -0.144, 0.994, 0.769, -0.438, 0.218, 0.808, 0.799, 0.417, 0.607, 0.164, -0.811, -0.569, 0.271, 0.745, -0.222, -0.476, -0.762, -0.473, 0.375, 0.534, -0.644, 0.131, -0.83, -0.859, -0.039, -0.668, -0.944, -0.285, -0.58, 0.463, 0.12, 0.366, 0.661, -0.375, -0.452, 0.597, 0.069, -0.665, -0.769, -0.491, 0.87, -0.977, -0.097, -0.949, 0.404, 0.45, 0.79, 0.599, -0.996, -0.788, 0.443, -0.404, 0.378, -0.206, 0.892, 0.941, -0.029, 0.006, -0.545, 0.202, 0.265, -0.015, 0.835, 0.237, -0.702, -0.219, -0.675, -0.925, -0.873, -0.497, -0.834, -0.022, -0.411, 0.295, -0.427, -0.966, 0.226, 0.01, 0.106, 0.695, -0.522, 0.124, -0.19, 0.835, 0.584, -0.756, -0.09, -0.017, -0.482, -0.964, -0.225, -0.339, 0.342, -0.99, -0.25, 0.4, 0.31, -0.491, 0.422, -0.029, -0.62, -0.133, 0.059, -0.664, -0.088, -0.451, -0.44, -0.193, 0.411, -0.619, -0.515, 0.999, -0.336, 0.908, 0.963, 0.856, -0.847, 0.699, -0.909, -0.562, -0.809, -0.495, -0.5, -0.958, 0.811, 0.424, -0.207, 0.12, -0.885, 0.306, -0.815, 0.083, -0.125, 0.134, 0.335, 0.233, -0.328, 0.543, -0.052, -0.398, -0.147, -0.43, 0.692, 0.862, -0.232, -0.333, 0.976, -0.567, 0.697, -0.787, 0.222, 0.438, 0.597, 0.318, 0.275, -0.183, 0.255, -0.822, 0.892, 0.663, -0.745, -0.691, 0.897, 0.325, 0.732, -0.092, 0.373, -0.774, -0.812, 0.677, 0.961, -0.667, -0.009, 0.421, -0.946, 0.6, 0.55, -0.714, -0.332, 0.882, 0.217, 0.812, -0.635, -0.125, -0.924, -0.023, -0.821, -0.431, 0.811, 0.128, 0.084, -0.11, 0.052, -0.119, 0.599, -0.693, 0.856, -0.075, -0.364, -0.774, 0.837, 0.69, 0.264, -0.207, 0.656, -0.613, 0.051, 0.958, 0.26, -0.781, 0.746, -0.947, 0.825, -0.154, 0.633, -0.139, -0.493, -0.66, 0.619, 0.041, 0.913, 0.962, 0.279, -0.019, -0.541, 0.284, -0.094, 0.412, 0.49, -0.878, 0.992, 0.256, -0.945, 0.759, 0.905, 0.241, -0.897, -0.155, -0.47, 0.245, 0.161, -0.681, -0.6, 0.721, 0.483, -0.992, -0.495, -0.858, -0.889, -0.141, -0.962, 0.139, 0.426, 0.255, 0.154, 0.129, 0.317, 0.196, 0.021, -0.253, 0.636, 0.186, -0.955, 0.279, -0.013, 0.682, -0.343, -0.653, -0.177, -0.528, 0.562, 0.78, -0.057, -0.615, -0.033, -0.686, -0.209, 0.436, -0.653, 0.427, 0.364, 0.82, 0.979, 0.819, 0.353, -0.005, 0.079, -0.642, -0.034, -0.58, -0.926, 0.95, -0.246, 0.039, -0.244, -0.144, 0.392, 0.853, 0.335, -0.851, 0.511, 0.183, -0.625, -0.213, -0.076, -0.864, -0.332, 0.84, 0.875, 0.518, 0.243, 0.5, 0.691, 0.438, 0.488, 0.996, 0.253, 0.015, -0.258, -0.725, 0.56, 0.886, 0.23, -0.451, 0.239, 0.096, -0.614, -0.02, 0.688, -0.269, -0.056, -0.524, -0.941, -0.077, -0.412, -0.878, -0.817, -0.681, 0.889, -0.388, 0.004, 0.764, -0.87, 0.944, 0.772, -0.681, -0.462, -0.146, -0.174, 0.516, 0.24, 0.071, 0.782, -0.848, -0.733, -0.259, 0.625, 0.745, 0.32, 0.419, -0.547, 0.157, -0.572, -0.327, -0.302, -0.357, 0.216, 0.674, -0.827, 0.561, -0.811, 0.791, 0.302, 0.728, 0.741, -0.11, 0.09, 0.893, -0.374, -0.232, -0.529, -0.35, 0.934, 0.289, -0.809, -0.162, -0.366, -0.717, -0.325, -0.504, 0.435, 0.034, 0.411, 0.253, 0.277, -0.078, 0.54, 0.498, -0.528, -0.707, 0.92, -0.625, -0.518, -0.14, 0.023, -0.533, 0.68, -0.402, 0.917, -0.385, -0.343, 0.784, 0.603, -0.388, -0.285, -0.658, -0.169, -0.285, -0.675, 0.615, -0.005, -0.807, 0.802, 0.879, 0.096, -0.634, -0.12, 0.529, -0.31, 0.143, -0.053, -0.749, -0.755, -0.326, 0.327, -0.602, 0.392, 0.87, 0.274, 0.934, -0.064, 0.694, 0.678, 0.0, -0.23, 0.499, -0.921, 0.342, -0.513, -0.439, -0.432, -0.621, -0.747, -0.605, 0.221, -0.549, 0.746, 0.92, 0.869, -0.553, -0.83, 0.939, 0.896, 0.08, -0.668, 0.527, 0.812, 0.624, -0.441, -0.986, -0.586, -0.287, 0.603, -0.48, -0.903, 0.599, 0.444, -0.193, -0.556, -0.603, 0.388, 0.245, -0.926, -0.49, 0.419, 0.091, 0.342, -0.757, 0.37, -0.591, 0.314, -0.357, 0.51, 0.509, 0.447, -0.762, 0.013, 0.512, 0.522, 0.261, -0.149, 0.516, -0.294, 0.626, 0.016, 0.89, -0.622, -0.536, 0.676, -0.012, 0.581, -0.6, 0.328, -0.295, -0.062, -0.287, -0.226, -0.462, 0.577, -0.745, 0.829, -0.764, 0.773, -0.987, -0.79, -0.032, 0.994, 0.745, -0.788, -0.182, -0.511, 0.127, 0.049, 0.886, 0.914, -0.532, -0.236, 0.766, -0.771, -0.726, -0.385, -0.648, -0.245, -0.364, -0.912, 0.288, 0.033, 0.38, 0.304, 0.598, -0.14, -0.148, -0.701, 0.367, 0.451, 0.958, -0.115, 0.063, 0.131, 0.103, 0.668, 0.864, 0.505, 0.225, -0.638, -0.334, -0.45, 0.942, -0.169, 0.339, 0.278, 0.279, 0.334, 0.862, 0.808, 0.035, 0.261, 0.395, 0.093, -0.771, 0.232, 0.513, -0.809, 0.786, 0.204, -0.253, 0.071, 0.818, -0.587, -0.594, -0.113, -0.025, 0.184, 0.074, 0.773, -0.475, 0.641, -0.814, 0.286, -0.139, -0.415, 0.125, 0.218, -0.977, -0.813, -0.78, 0.054, 0.163, 0.933, 0.556, 0.863, 0.49, 0.688, -0.219, -0.803, 0.212, -0.834, -0.675, 0.127, -0.099, -0.41, 0.231, -0.7, 0.861, -0.576, -0.422, 0.23, -0.865, 0.527, -0.167, -0.055, -0.801, -0.338, -0.951, -0.558, -0.093, 0.349, -0.95, 0.337, -0.546, -0.199, 0.073, -0.018, -0.908, 0.654, 0.111, 0.578, 0.317, -0.123, 0.432, -0.324, -0.206, -0.651, 0.999, 0.571, -0.41, 0.933, -0.984, 0.498, -0.531, 0.068, 0.641, -0.482, 0.131, 0.505, 0.523, 0.857, -0.084, -0.48, -0.766, 0.091, -0.732, 0.436, -0.985, 0.571, 0.455, -0.739, 0.631, 0.704, -0.103, 0.766, -0.672, -0.576, -0.078, -0.495, -0.07, 0.133, 0.661, -0.305, 0.774, 0.98, 0.089, -0.343, -0.241, 0.388, 0.26, 0.584, 0.974, 0.132, 0.262, 0.944, 0.25, -0.635, 0.652, 0.208, 0.547, 0.67, 0.339, 0.699, -0.95, -0.085, 0.14, 0.476, 0.71, -0.513, -0.473, 0.847, 0.281, 0.703, -0.405, -0.107, 0.623, -0.305, -0.129, 0.528, 0.272, 0.283, 0.228, 0.955, 0.344, 0.917, 0.361, 0.22, 0.389, 0.85, 0.676, -0.062, 0.397, 0.767, -0.767, -0.304, 0.482, 0.347, -0.921, 0.09, 0.034, 0.085, 0.224, 0.541, -0.6, -0.297, -0.622, 0.072, 0.588, -0.082, 0.685, -0.426, -0.603, -0.662, 0.261, 0.162, -0.817, -0.691, -0.502, -0.747, -0.056, -0.998, 0.005, 0.831, 0.054, 0.858, 0.965, -0.102, -0.783, -0.829, 0.082, 0.491, -0.038, -0.306, -0.5, -0.355, -0.3, -0.445, 0.391, -0.618, -0.271, -0.998, 0.085, 0.001, -0.586, 0.806, -0.894, -0.019, -0.51, -0.453, -0.516, 0.481, -0.474, 0.379, 0.683, 0.321, 0.603, 0.64, 0.193, -0.84, -0.678, -0.364, -0.594, 0.854, -0.923, -0.054, -0.525, -0.937, -0.282, -0.237, -0.949, 0.496, 0.019, 0.247, -0.936, 0.502, -0.487, -0.023, 0.22, 0.083, -0.059, -0.023, -0.316, 0.757, -0.631, 0.863, 0.04, 0.842, -0.29, -0.607, -0.103, 0.157, 0.484, -0.127, -0.95, -0.834, 0.175, 0.056, -0.933, -0.901, 0.452, 0.203, 0.002, 0.741, -0.454, -0.782, -0.737, 0.341, -0.152, 0.009, 0.068, -0.779, -0.91, 0.663, -0.516, -0.623, 0.198, -0.428, 0.473, 0.501, -0.125, -0.447, 0.01, 0.587, 0.071, -0.565, 0.467, 0.905, 0.699, 0.458, -0.728, 0.723, -0.418, -0.257, 0.312, 0.814, -0.715, 0.383, 0.062, -0.288, -0.772, -0.585, -0.451, 0.77, 0.698, -0.935, 0.284, 0.819, 0.168, 0.543, -0.126, -0.255, 0.138, -0.689, -0.759, -0.824, -0.632, -0.835, -0.683, 0.074, 0.771, 0.862, -0.755, -0.099, 0.22, -0.041, -0.597, 0.27, -0.263, -0.86, 0.081, 0.163, -0.378, 0.831, 0.808, -0.208, -0.765, -0.804, -0.964, 0.791, -0.578, 0.838, 0.471, 0.79, 0.303, -0.914, 0.497, 0.981, -0.678, 0.489, -0.13, -0.72, -0.581, 0.936, -0.138, 0.319, 0.501, 0.849, -0.334, 0.523, 0.86, 0.281, -0.495, -0.117, -0.903, 0.352, -0.465, 0.523, -0.826, 0.655, 0.781, 0.349, 0.858, -0.976, -0.859, -0.352, -0.203, 0.234, 0.153, -0.51, -0.408, -0.26, -0.381, 0.908, -0.03, 0.256, -0.169, -0.244, 0.451, 0.08, 0.817, 0.725, -0.343, -0.194, -0.578, 0.267, -0.975, 0.409, -0.908, 0.517, 0.759, -0.704, -0.266, 0.783, -0.952, -0.052, 0.956, -0.273, -0.641, -0.639, -0.881, -0.316, 0.157, -0.758, 0.837, -0.66, -0.469, 0.699, 0.613, -0.527, 0.439, 0.635, -0.107, -0.367, 0.613, -0.027, -0.124, 0.087, 0.0, -0.784, -0.813, 0.059, -0.121, -0.371, -0.76, 0.524, -0.441, -0.541, -0.705, -0.997, 0.613, 0.432, 0.888, -0.01, -0.4, -0.472, -0.003, -0.527, 0.928, 0.595, -0.592, 0.269, -0.166, 0.277, 0.893, 0.62, -0.861, 0.429, -0.222, 0.529, -0.014, 0.953, 0.328, 0.805, 0.59, 0.091, -0.549, -0.505, 0.071, 0.221, 0.526, 0.355, -0.664, -0.126, -0.374, 0.634, -0.903, -0.411, 0.884, 0.01, -0.67, -0.838, -0.247, -0.971, -0.875, -0.523, 0.344, 0.095, 0.58, 0.51, -0.666, 0.516, -0.39, -0.53, -0.617, 0.182, 0.582, 0.037, 0.078, -0.686, -0.768, 0.748, 0.069, 0.06, 0.229, -0.06, -0.204, -0.963, 0.604, -0.646, -0.028, 0.018, 0.946, -0.174, 0.017, -0.995, 0.876, 0.063, 0.808, -0.327, 0.101, 0.053, -0.152, -0.374, 0.106, -0.018, 0.806, -0.634, 0.285, 0.46, 0.547, 0.983, -0.143, 0.924, -0.536, -0.354, -0.515, -0.715, 0.23, -0.046, -0.718, 0.171, -0.021, 0.102, -0.586, -0.367, -0.055, 0.053, 0.324, 0.225, 0.269, 0.805, -0.961, 0.563, 0.646, -0.729, -0.192, 0.922, -0.409, 0.921, -0.449, 0.524, -0.949, 0.197, -0.168, -0.677, 0.016, 0.556, 0.023, -0.968, 0.096, 0.994, 0.008, 0.235, -0.664, 0.33, 0.155, 0.97, 0.706, -0.013, -0.258, -0.033, 0.58, 0.046, -0.268, 0.044, 0.354, 0.667, 0.745, 0.594, -0.993, 0.914, 0.283, -0.339, 0.069, -0.982, -0.029, -0.241, 0.868, 0.894, 0.579, -0.171, -0.794, -0.059, -0.695, 0.287, -0.154, 0.422, -0.031, -0.454, -0.679, -0.261, -0.464, -0.592, -0.192, -0.981, -0.692, 0.096, 0.768, -0.394, 0.871, 0.849, 0.786, -0.803, 0.511, -0.251, -0.992, 0.764, 0.731, 0.63, -0.643, -0.028, 0.257, 0.798, -0.031, -0.261, 0.377, 0.79, -0.287, -0.723, 0.729, -0.161, -0.115, -0.961, 0.45, -0.001, -0.524, 0.478, -0.442, 0.421, -0.98, -0.466, 0.598, 0.875, 0.505, 0.404, -0.017, -0.011, 0.814, -0.262, 0.445, 0.844, 0.562, -0.547, 0.661, -0.33, -0.345, -0.351, -0.78, 0.4, -0.324, -0.192, 0.025, 0.542, -0.903, 0.484, -0.364, 0.77, -0.993, -0.655, -0.136, 0.329, 0.384, -0.352, 0.571, 0.607, 0.118, 0.301, -0.8, -0.437, -0.103, -0.492, 0.993, 0.779, 0.277, -0.838, -0.103, -0.463, 0.061, -0.241, -0.15, 0.404, 0.729, 0.077, 0.899, 0.59, -0.19, 0.593, 0.014, -0.009, 0.713, -0.727, -0.196, 0.123, -0.809, -0.37, -0.164, -0.807, 0.975, 0.898, 0.261, 0.927, 0.356, -0.75, -0.724, 0.479, -0.778, -0.611, -0.679, -0.563, 0.922, 0.354, 0.584, 0.806, 0.025, 0.696, -0.783, 0.53, 0.01, 0.027, 0.475, 0.149, -0.494, -0.477, 0.802, -0.423, 0.003, 0.371, -0.015, -0.413, -0.063, 0.416, 0.799, 0.927, -0.252, -0.921, -0.846, -0.715, -0.749, 0.755, 0.604, 0.809, -0.337, 0.513, 0.297, -0.316, 0.599, 0.387, -0.243, -0.728, 0.575, 0.209, 0.071, 0.835, -0.739, 0.822, -0.013, -0.503, -0.542, 0.453, 0.789, 0.619, -0.139, -0.155, 0.451, 0.322, 0.393, 0.796, 0.055, 0.816, -0.312, 0.696, -0.809, -0.06, 0.989, 0.863, -0.822, 0.92, 0.523, -0.711, 0.699, 0.489, 0.071, 0.958, 0.894, -0.052, -0.804, 0.227, 0.143, 0.432, 0.45, 0.597, 0.431, -0.689, 0.608, -0.617, 0.263, -0.568, -0.634, -0.838, -0.57, 0.846, -0.429, -0.924, 0.719, -0.332, 0.352, -0.773, -0.297, 0.348, 0.626, 0.698, -0.664, -0.895, 0.376, 0.39, 0.136, -0.049, -0.673, -0.535, -0.314, -0.424, -0.61, -0.811, 0.273, 0.355, -0.16, -0.946, -0.692, -0.317, 0.398, -0.382, -0.073, -0.423, 0.093, 0.847, -0.114, 0.312, 0.528, -0.247, -0.197, 0.273, 0.961, 0.739, -0.774, 0.477, -0.924, 0.674, 0.526, 0.803, -0.644, 0.123, -0.13, 0.926, 0.706, 0.052, 0.372, 0.955, -0.311, -0.224, 0.194, 0.446, -0.485, 0.618, -0.528, 0.868, -0.872, 0.376, 0.707, 0.33, 0.459, -0.037, -0.032, 0.264, 0.078, -0.619, 0.707, -0.131, 0.68, -0.04, 0.693, -0.338, 0.763, -0.743, -0.694, -0.151, -0.948, 0.522, -0.033, 0.632, 0.905, -0.99, -0.008, -0.05, 0.64, -0.702, -0.526, 0.925, 0.324, 0.92, 0.919, -0.736, 0.01, -0.164, -0.039, 0.846, -0.667, 0.143, 0.292, -0.413, -0.583, 0.634, -0.357, 0.56, 0.377, -0.812, -0.729, 0.827, -0.393, 0.567, 0.096, -0.705, -0.943, 0.212, 0.151, -0.649, 0.525, -0.561, 0.77, -0.687, 0.962, -0.964, 0.632, -0.334, 0.541, -0.205, 0.541, -0.648, -0.869, 0.23, -0.846, 0.611, 0.018, 0.519, -0.966, 0.043, -0.485, 0.502, -0.723, 0.309, -0.866, -0.046, -0.583, -0.739, 0.602, 0.786, -0.434, -0.087, -0.592, 0.151, -0.969, -0.713, 0.629, 0.52, -0.916, -0.221, 0.348, 0.846, 0.618, 0.173, 0.698, -0.121, -0.181, 0.085, -0.394, 0.181, -0.732, -0.952, 0.766, 0.657, -0.239, 0.606, 0.789, -0.868, -0.817, -0.956, 0.087, -0.588, 0.725, -0.545, -0.457, 0.486, 0.947, -0.979, -0.133, -0.095, -0.329, -0.566, 0.24, -0.601, -0.238, -0.45, -0.219, 0.176, -0.289, 0.308, 0.972, -0.796, -0.409, 0.444, 0.131, 0.943, 0.024, 0.809, 0.452, -0.112, 0.864, 0.709, -0.701, 0.22, -0.193, 0.792, 0.437, -0.877, 0.745, -0.79, 0.862, -0.824, -0.621, 0.426, -0.703, 0.825, 0.304, 0.17, 0.642, -0.507, -0.178, 0.712, -0.845, -0.846, -0.253, -0.595, 0.886, 0.657, 0.737, -0.515, -0.311, -0.048, -0.442, 0.65, -0.375, -0.859, -0.356, 0.78, -0.581, 0.392, 0.115, 0.962, -0.678, -0.559, 0.052, 0.377, -0.275, 0.674, 0.465, 0.493, -0.566, -0.928, -0.993, -0.12, -0.284, -0.244, 0.982, 0.209, 0.465, 0.752, 0.054, 0.987, -0.532, 0.087, -0.493, 0.723, -0.093, 0.536, 0.198, 0.304, -0.447, -0.506, 0.482, 0.892, 0.603, 0.224, 0.261, -0.183, 0.701, -0.127, -0.891, -0.934, 0.559, 0.048, -0.203, 0.452, 0.615, 0.045, -0.674, 0.753, 0.017, -0.964, -0.461, 0.278, -0.375, 0.995, -0.1, 0.703, 0.622, 0.467, -0.151, -0.547, -0.165, -0.453, 0.227, 0.407, 0.631, -0.52, 0.47, -0.173, -0.222, -0.656, 0.537, -0.89, -0.332, 0.779, 0.089, 0.91, -0.978, -0.356, 0.292, -0.608, -0.711, 0.354, -0.563, -0.554, 0.41, -0.504, -0.276, -0.377, -0.914, 0.449, -0.298, 0.594, -0.853, -0.056, 0.653, 0.196, 0.399, 0.181, -0.684, -0.602, 0.417, 0.888, -0.551, 0.563, -0.138, -0.205, -0.487, 0.508, 0.019, 0.625, -0.172, 0.604, -0.541, 0.52, -0.826, -0.013, 0.611, 0.205, 0.128, -0.948, 0.519, -0.817, 0.645, -0.644, 0.026, -0.007, 0.973, 0.617, 0.429, -0.549, 0.207, -0.255, -0.318, -0.789, 0.808, 0.555, -0.303, -0.655, -0.129, -0.812, 0.254, -0.558, -0.021, -0.707, -0.749, 0.21, -0.201, -0.511, -0.021, -0.634, 0.112, 0.06, -0.717, -0.48, 0.018, -0.168, 0.184, -0.585, 0.96, -0.307, 0.072, -0.626].__iter__()
  
  def generate(self, iterable):
    if not self.INIT_WEIGHTS:
      return [random.random() * 2 - 1 for it in iterable]
    else:
      return [random.random() * 0 + self.INIT_WEIGHTS.__next__() for it in iterable]

class INeuron:
  def __init__(self, activator, initializer, previous_layer): pass
  def calculate(self):       pass
  def sprintf_weights(self): pass

class InputValue(INeuron):
  def __init__(self, activator, initializer, previous_layer):
    self.activator = activator
    self.coef = initializer.generate(' ')[0]
    self.value = 0
    self.cache = None
    self.coefs = [self.coef]
    self.next_layer = []
  
  @catch_nan
  def calculate(self):
    if not self.cache:
      self.cache = self.activator.result(self.value)
    return self.cache
  
  @catch_nan
  def delta_as_last(self, error):
    s = self.coef * self.value
    d = self.activator.derivative(s)
    
    return d * error
  
  @catch_nan
  def delta_as_not_last(self, next_deltas):
    s = self.coef * self.value
    d = self.activator.derivative(s)
    
    mult = 0
    for i, neuron in enumerate(self.next_layer):
      mult += neuron[0].coefs[neuron[1]] * next_deltas[i]
    
    return d * mult
  
  @catch_nan
  def delta(self, next_deltas):
    if self.next_layer:
      a = self.delta_as_not_last(next_deltas)
    else:
      a = self.delta_as_last(next_deltas)
    
    train_value = TRAIN_SPEED * a * self.value
    
    if train_value < -TRAIN_LIMIT: train_value = -TRAIN_LIMIT
    elif train_value > TRAIN_LIMIT: train_value = TRAIN_LIMIT
    
    k = self.coef - train_value
    if k < -COEF_LIMIT: k = -COEF_LIMIT
    elif k > COEF_LIMIT: k = COEF_LIMIT
    self.coef = k
    self.coefs = [k]
    self.cache = None
    
    return a

class Neuron(INeuron):
  def __init__(self, activator, initializer, previous_layer):
    self.activator = activator
    self.previous_layer = previous_layer
    self.next_layer = []
    self.coefs = initializer.generate(previous_layer)
    self.cache = None
    
    for i,p in enumerate(self.previous_layer):
      p.next_layer.append((self, i))
  
  @catch_nan
  def calculate(self):
    if not self.cache:
      s = sum(self.coefs[i] * v.calculate() for i, v in enumerate(self.previous_layer))
      self.cache = self.activator.result(s)
    
    return self.cache
  
  @catch_nan
  def delta_as_last(self, error):
    s = sum(self.coefs[i] * v.calculate() for i, v in enumerate(self.previous_layer))
    
    d = self.activator.derivative(s)
    
    return d * error
  
  @catch_nan
  def delta_as_not_last(self, next_deltas):
    s = sum(self.coefs[i] * v.calculate() for i, v in enumerate(self.previous_layer))
    
    d = self.activator.derivative(s)
    
    mult = 0
    for i, neuron in enumerate(self.next_layer):
      mult += neuron[0].coefs[neuron[1]] * next_deltas[i]
    
    return d * mult
  
  @catch_nan
  def delta(self, next_deltas):
    if self.next_layer:
      a = self.delta_as_not_last(next_deltas)
    else:
      a = self.delta_as_last(next_deltas)
    
    for i, prev_neuron in enumerate(self.previous_layer):
      train_value = TRAIN_SPEED * a * prev_neuron.calculate()
      
      if train_value < -TRAIN_LIMIT: train_value = -TRAIN_LIMIT
      elif train_value > TRAIN_LIMIT: train_value = TRAIN_LIMIT
      
      k = self.coefs[i] - train_value
      if k < -COEF_LIMIT: k = -COEF_LIMIT
      elif k > COEF_LIMIT: k = COEF_LIMIT
      self.coefs[i] = k
    
    self.cache = None
    
    return a

class SparelinkNeuralNetwork:
  def __init__(self, activator, initializer, levels):
    self.activator_prefix = str(activator)
    
    self.layers = [[InputValue(activator, initializer, None) for i in range(4 ** levels * 3)]]
    
    # first layer (4 ** levels == 256) - neurons with 3 inputs (R, G, B)
    self.layers.append(
      [Neuron(activator, initializer, self.layers[0][i*3:i*3+3]) for i in range(4 ** levels)]
    )
    
    # second layer (4 ** 3 == 64)
    # third layer  (4 ** 2 == 16)
    # fourth layer (4 ** 1 == 4)
    # final layer  (4 ** 0 == 1)
    for i in range(levels)[::-1]:
      last_layer = self.layers[-1]
      size = 4 ** i
      
      self.layers.append([Neuron(activator, initializer, [
        last_layer[k], last_layer[k + size], last_layer[k + 2 * size], last_layer[k + 3 * size]
      ]) for k in range(size)])
    
  def set_inputs(self, input_values):
    for i, neuron in enumerate(self.layers[0]):
      neuron.value = input_values[i]
    
    for layer in self.layers:
      for neuron in layer:
        neuron.cache = None
  
  def calculate(self):
    for neuron in self.layers[-1]:
      yield neuron.calculate()
  
  def train(self, wanted):
    # back errors propagation
    output = list(self.calculate())
    
    # output layer
    deltas = [
      neuron.delta(output[i] - wanted[i])
          for i, neuron in enumerate(self.layers[-1])
    ]
    
    for layer in self.layers[:-1][::-1]:
      deltas = [neuron.delta(deltas) for neuron in layer]
  
  def enum_weights(self):
    for layer in self.layers:
      for neuron in layer:
        yield from neuron.coefs

def epoch(net, data):
  sum_sq = 0
  
  cases = list(range(data.cases()))
  random.shuffle(cases)
  
  trains = data.cases() // 2
  
  for case in cases:
    net.set_inputs(data.extract_data(case))
    
    net_result = net.calculate()
    wanted_result = data.wanted(case)
    
    for i, nr in enumerate(net_result):
      sum_sq += (nr - wanted_result[i]) * (nr - wanted_result[i])
    
    if trains > 0:
      net.train(wanted_result)
      trains -= 1
    
  return sum_sq / data.cases()

def main():
  try:
    random.seed(0x14609A25)
    
    net = SparelinkNeuralNetwork(TanhActivator(), InitialWeightsGenerator(), 4)
    data = IconDataExtractor(__file__ + '/../icons/r-',
      ['oo-0-0.png',   'tg-0-0.png',   'ya-0-0.png',
       'oo-90-0.png',  'tg-90-0.png',  'ya-90-0.png',
       'oo-180-0.png', 'tg-180-0.png', 'ya-180-0.png',
       'oo-270-0.png', 'tg-270-0.png', 'ya-270-0.png',
      ])
    
    print(net, data)
    last_distance = epoch(net, data)
    
    try:
      for i in range(100001):
        cur_distance = epoch(net, data)
        
        if i % 200 == 0:
          print('Epoch %6d - square distance = %.4f (delta = %.4f)' % (i, cur_distance, cur_distance - last_distance))
          last_distance = min(last_distance, cur_distance)
        
        if cur_distance < 0.005:
          break
    except KeyboardInterrupt:
      pass
    except Exception:
      raise
    
    print('\nResults:')
    
    sum_distance = 0
    sum_abs = 0
    
    for case in range(data.cases()):
      net.set_inputs(data.extract_data(case))
      
      net_result = list(net.calculate())
      wanted_result = data.wanted(case)
      
      print('Net output = %.4f (%d); wanted = %d' % (net_result[0], net_result[0], wanted_result[0]))
      
      for i, nr in enumerate(net_result):
        sum_distance += (nr - wanted_result[i]) * (nr - wanted_result[i])
        sum_abs += abs(nr - wanted_result[i])
    
    print('\nAverage square error: %.4f' % (sum_distance / data.cases()))
    print('Average absolute error: %.4f' % (sum_abs / data.cases()))
    
    print('\nWeights:')
    print([int(w * 1000) / 1000 for w in net.enum_weights()])
  except:
    traceback.print_exc()

main()
input()
